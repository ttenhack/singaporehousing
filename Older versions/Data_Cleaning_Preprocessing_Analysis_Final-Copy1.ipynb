{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "<hr>\n",
    "The libraries used are pandas, numpy, seaborn, Counter, matplotlib, axes3d, linearregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from IPython.display import IFrame\n",
    "import geocoder\n",
    "\n",
    "# machine learning library\n",
    "from sklearn import datasets, linear_model, cross_validation\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# other\n",
    "import time\n",
    "\n",
    "pd.options.mode.chained_assignment = None #SettingWithCopyWarning for confusing chained assignment disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Acquisition\n",
    "<hr>\n",
    "The data for this report is acquired from the Singapore government website [1]. Data are collected from the period 1990 until January 2018. The data is provided in four seperate files, which will be merged into Python. The third file (> 20 MB) was seperated into periods of 2006-2012 and 2012-2014. This was necessary to make use of the Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data1 = pd.read_csv('sg-resale-flat-prices-1990-1999.csv', sep =',')\n",
    "data2 = pd.read_csv('sg-resale-flat-prices-2000-2005.csv', sep =',')\n",
    "data3 = pd.read_csv('sg-resale-flat-prices-2006-2012.csv', sep =',')\n",
    "data4 = pd.read_csv('sg-resale-flat-prices-2012-2014.csv', sep =',')\n",
    "data5 = pd.read_csv('sg-resale-flat-prices-2014-2018.csv', sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data = (58631, 10)\n"
     ]
    }
   ],
   "source": [
    "data5 = data5.drop('remaining_lease',1)\n",
    "print('Number of training data =', data5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data = (768629, 10)\n"
     ]
    }
   ],
   "source": [
    "#concatenate dataset\n",
    "sets = [data1, data2, data3, data4, data5]\n",
    "data = pd.concat(sets)\n",
    "print('Number of training data =', data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Cleaning and Preprocessing the Dataset\n",
    "<hr>\n",
    "The cleaning and preprocessing section are divided into four sections, namely cleaning, encoding, feature engineering and one hot encoding.\n",
    "\n",
    "### 3.1 Data Cleaning\n",
    "During the exploration, there are some cleaning that should be performed. First, the flat types consist of eight types, which should be seven types instead. The flat type \"Multi Generation\" has a unique value with a space in between and one with a hyphen. Second, the flat models consist of 32 models, which should be 21 instead. This is also because of the capital usage. These doubles are removed by cleaning the data. Third, some storey range values in data set 4 are not correct. In the exploration, we have research the difference and since *** these values will be too different, these data points have been removed from the data set. Fourth, the feature \"month\" consists of sales year and sales month, e.g. 1990-01. To include the years and months in the model, this variable will be seperated to a variable called sales year and a variable called sales month. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Flat Type Count: 7\n",
      "Flat Type \n",
      "4 ROOM              285136\n",
      "3 ROOM              258482\n",
      "5 ROOM              156260\n",
      "EXECUTIVE            58177\n",
      "2 ROOM                8859\n",
      "1 ROOM                1246\n",
      "MULTI GENERATION       469\n",
      "Name: flat_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None #SettingWithCopyWarning for confusing chained assignment disabled\n",
    "\n",
    "#remove doubles\n",
    "data['flat_type'][data['flat_type'] == 'MULTI-GENERATION'] = 'MULTI GENERATION'\n",
    "\n",
    "#flat_type count\n",
    "count_flat_type = data['flat_type'].nunique()\n",
    "print(\"Total Flat Type Count:\", count_flat_type)\n",
    "flat_type_count = data['flat_type'].value_counts()\n",
    "print(\"Flat Type \\n\" +str(flat_type_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Flat Model Count: 21\n",
      "Flat Model Count \n",
      "Model A                   208633\n",
      "Improved                  202602\n",
      "New Generation            169643\n",
      "Simplified                 51604\n",
      "Standard                   38234\n",
      "Premium Apartment          28886\n",
      "Maisonette                 25136\n",
      "Apartment                  19745\n",
      "Apertment                   9901\n",
      "Model A2                    8382\n",
      "Adjoined flat               1913\n",
      "Model A-Maisonette          1784\n",
      "Terrace                      609\n",
      "DBSS                         601\n",
      "Multi Generation             469\n",
      "Type S1                      183\n",
      "Improved-Maisonette          105\n",
      "Type S2                       80\n",
      "Premium Maisonette            75\n",
      "2-room                        38\n",
      "Premium Apartment Loft         6\n",
      "Name: flat_model, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#remove doubles\n",
    "data['flat_model'][data['flat_model'] == 'MODEL A'] = 'Model A'\n",
    "data['flat_model'][data['flat_model'] == 'IMPROVED'] = 'Improved'\n",
    "data['flat_model'][data['flat_model'] == 'NEW GENERATION'] = 'New Generation'\n",
    "data['flat_model'][data['flat_model'] == 'PREMIUM APARTMENT'] = 'Premium Apartment'\n",
    "data['flat_model'][data['flat_model'] == 'SIMPLIFIED'] = 'Simplified'\n",
    "data['flat_model'][data['flat_model'] == 'STANDARD'] = 'Standard'\n",
    "data['flat_model'][data['flat_model'] == 'APARTMENT'] = 'Apertment'\n",
    "data['flat_model'][data['flat_model'] == 'MAISONETTE'] = 'Maisonette'\n",
    "data['flat_model'][data['flat_model'] == 'ADJOINED FLAT'] = 'Adjoined flat'\n",
    "data['flat_model'][data['flat_model'] == 'MODEL A-MAISONETTE'] = 'Model A-Maisonette'\n",
    "data['flat_model'][data['flat_model'] == 'TERRACE'] = 'Terrace'\n",
    "data['flat_model'][data['flat_model'] == 'MULTI GENERATION'] = 'Multi Generation'\n",
    "data['flat_model'][data['flat_model'] == 'IMPROVED-MAISONETTE'] = 'Improved-Maisonette'\n",
    "data['flat_model'][data['flat_model'] == '2-ROOM'] = '2-room'\n",
    "\n",
    "#flat_model count\n",
    "count_flat_model = data['flat_model'].nunique()\n",
    "print(\"Total Flat Model Count:\", count_flat_model)\n",
    "flat_model_count = data['flat_model'].value_counts()\n",
    "print(\"Flat Model Count \\n\" +str(flat_model_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Storey Range Count: 17\n",
      "Storey Range Count \n",
      "04 TO 06    196169\n",
      "07 TO 09    177012\n",
      "01 TO 03    158446\n",
      "10 TO 12    149470\n",
      "13 TO 15     46780\n",
      "16 TO 18     16906\n",
      "19 TO 21      8337\n",
      "22 TO 24      5233\n",
      "25 TO 27      2100\n",
      "28 TO 30       788\n",
      "34 TO 36       151\n",
      "31 TO 33       151\n",
      "37 TO 39       148\n",
      "40 TO 42        73\n",
      "46 TO 48        11\n",
      "43 TO 45        11\n",
      "49 TO 51         5\n",
      "Name: storey_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#remove storey range outliers\n",
    "#data = data.ix[data['storey_range'].isin(['01 TO 05','06 TO 10','11 TO 15','16 TO 20','21 TO 25','26 TO 30','31 TO 35','36 TO 40'])]\n",
    "data = data.loc[data['storey_range'].isin(['01 TO 03','04 TO 06','07 TO 09','10 TO 12','13 TO 15','16 TO 18','19 TO 21','22 TO 24','25 TO 27','28 TO 30','31 TO 33','34 TO 36','37 TO 39','40 TO 42','43 TO 45','46 TO 48','49 TO 51'])]\n",
    "\n",
    "#storey range count\n",
    "count_storey_range = data['storey_range'].nunique()\n",
    "print(\"Total Storey Range Count:\", count_storey_range)\n",
    "storey_range_count = data['storey_range'].value_counts()\n",
    "print(\"Storey Range Count \\n\" +str(storey_range_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add sales year variable\n",
    "if ('sales_year' not in data.columns):\n",
    "    data.insert(1,'sales_year',(pd.DatetimeIndex(data['month']).year))\n",
    "\n",
    "#add sales year variable\n",
    "if ('sales_month' not in data.columns):\n",
    "    data.insert(1,'sales_month',(pd.DatetimeIndex(data['month']).month))\n",
    "    \n",
    "#add sales year variable\n",
    "if ('month' in data.columns):\n",
    "    del data['month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that two variables are not going to be used, which are respectively block and street name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_month</th>\n",
       "      <th>sales_year</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>flat_model</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>1977</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>1977</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>73.0</td>\n",
       "      <td>New Generation</td>\n",
       "      <td>1976</td>\n",
       "      <td>47200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_month  sales_year        town flat_type storey_range  floor_area_sqm  \\\n",
       "0            1        1990  ANG MO KIO    1 ROOM     10 TO 12            31.0   \n",
       "1            1        1990  ANG MO KIO    1 ROOM     04 TO 06            31.0   \n",
       "2            1        1990  ANG MO KIO    1 ROOM     10 TO 12            31.0   \n",
       "3            1        1990  ANG MO KIO    1 ROOM     07 TO 09            31.0   \n",
       "4            1        1990  ANG MO KIO    3 ROOM     04 TO 06            73.0   \n",
       "\n",
       "       flat_model  lease_commence_date  resale_price  \n",
       "0        Improved                 1977        9000.0  \n",
       "1        Improved                 1977        6000.0  \n",
       "2        Improved                 1977        8000.0  \n",
       "3        Improved                 1977        6000.0  \n",
       "4  New Generation                 1976       47200.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove unnecessary variables\n",
    "data = data.drop('block',1)\n",
    "data = data.drop('street_name',1)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encoding\n",
    "When using categorical data, strings are not able to be interpreted by algorithms. Therefore, these values needs to be translated to a numerical value. For example, the towns in the dataset will be translated to $1,2,…,n$. Since there are rows in our dataset containing characters. These rows (town, flat type, flat model and storey range) are transformed into dummy variables to clarify their levels, with other words, to quantify the qualitative data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that data.copy() is used to make a copy of data, which will be used for analysis\n",
    "data_enc = data.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies for town\n",
    "town_array = np.unique(data['town'])\n",
    "n = len(town_array)\n",
    "\n",
    "for i in range(0,n):\n",
    "    data_enc['town'][data['town'] == town_array[i]] = i+1\n",
    "\n",
    "#count_town = data['town'].nunique()\n",
    "#print(\"Total Town Count:\", count_town)\n",
    "#town_count = data['town'].value_counts()\n",
    "#print(\"Town Count \\n\" +str(town_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies for flat types\n",
    "data_enc['flat_type'][data.flat_type == '1 ROOM'] = 1\n",
    "data_enc['flat_type'][data.flat_type == '2 ROOM'] = 2\n",
    "data_enc['flat_type'][data.flat_type == '3 ROOM'] = 3\n",
    "data_enc['flat_type'][data.flat_type == '4 ROOM'] = 4\n",
    "data_enc['flat_type'][data.flat_type == '5 ROOM'] = 5\n",
    "data_enc['flat_type'][data.flat_type == 'MULTI GENERATION'] = 6\n",
    "data_enc['flat_type'][data.flat_type == 'EXECUTIVE'] = 7\n",
    "\n",
    "#flat_type_count = data['flat_type'].value_counts()\n",
    "#print(\"Flat Type \\n\" +str(flat_type_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies for storey ranges \n",
    "storey_range_array = np.unique(data['storey_range'])\n",
    "n = len(storey_range_array)\n",
    "\n",
    "for i in range(0,n):\n",
    "    data_enc['storey_range'][data['storey_range'] == storey_range_array[i]] = i+1\n",
    "\n",
    "#count_storey_range = data['storey_range'].nunique()\n",
    "#print(\"Total Storey Range Count:\", count_storey_range)\n",
    "#storey_range_count = data['storey_range'].value_counts()\n",
    "#print(\"Storey Range Count \\n\" +str(storey_range_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies for flat models\n",
    "flat_model_array = np.unique(data['flat_model'])\n",
    "n = len(flat_model_array)\n",
    "\n",
    "for i in range(0,n):\n",
    "    data_enc['flat_model'][data['flat_model'] == flat_model_array[i]] = i+1\n",
    "\n",
    "#count_flat_model = data['flat_model'].nunique()\n",
    "#print(\"Total Flat Model Count:\", count_flat_model)\n",
    "#flat_model_count = data['flat_model'].value_counts()\n",
    "#print(\"Flat Model Count \\n\" +str(flat_model_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_month</th>\n",
       "      <th>sales_year</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>flat_model</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1977</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1977</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>73.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1976</td>\n",
       "      <td>47200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_month  sales_year town flat_type storey_range  floor_area_sqm  \\\n",
       "0            1        1990    1         1            4            31.0   \n",
       "1            1        1990    1         1            2            31.0   \n",
       "2            1        1990    1         1            4            31.0   \n",
       "3            1        1990    1         1            3            31.0   \n",
       "4            1        1990    1         3            2            73.0   \n",
       "\n",
       "  flat_model  lease_commence_date  resale_price  \n",
       "0          6                 1977        9000.0  \n",
       "1          6                 1977        6000.0  \n",
       "2          6                 1977        8000.0  \n",
       "3          6                 1977        6000.0  \n",
       "4         13                 1976       47200.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 One Hot Encoding (Categorical Data)\n",
    "Label encoding is a traditional way of translating strings into numerical values. The disadvantage of this method is the fact that algorithms might misinterpret these values. A higher town value does not necessarily mean that it has the potential of having higher resale prices.\n",
    "\n",
    "To cope with this problem, the one hot encoding approach is utilised. Instead of giving a numerical value, new columns are created per feature value. Continuing with the town example, this would mean that every town would have a new column. When the datapoint is part of this value, it will receive a $1$, whilst the other values receive a $0$ in this column. Therefore, it can be considered as a boolean solution for the feature values; either it is part of the value (True) or it is not (False). The disadvantage of the method is the fact that a significant amount of columns will be added to the dataset.\n",
    "\n",
    "To use the one hot encoding approach, the Panda feature get_dummies is used. This is similar to the LabelBinarizer function used in the Scikit-learn package. We chose for the pandas approach as our data was already converted to a pandas DataFrame. One hot encoding are used for the following features: town/area, flat type, flat model and storey range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that data.copy() is used to make a copy of data, which will be used for analysis\n",
    "data_henc = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for town\n",
    "dummies = pd.get_dummies(data_henc['town']).rename(columns=lambda x: 'town_' + str(x))\n",
    "data_henc = pd.concat([data_henc, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for flat types\n",
    "dummies = pd.get_dummies(data_henc['flat_type']).rename(columns=lambda x: 'flat_type_' + str(x))\n",
    "data_henc = pd.concat([data_henc, dummies], axis=1)\n",
    "\n",
    "#source: http://www.hdb.gov.sg/cs/infoweb/residential/buying-a-flat/new/types-of-flats&rendermode=preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for storey ranges\n",
    "dummies = pd.get_dummies(data_henc['storey_range']).rename(columns=lambda x: 'storey_range_' + str(x))\n",
    "data_henc = pd.concat([data_henc, dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for flat models\n",
    "dummies = pd.get_dummies(data_henc['flat_model']).rename(columns=lambda x: 'flat_model_' + str(x))\n",
    "data_henc = pd.concat([data_henc, dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761791, 77)\n"
     ]
    }
   ],
   "source": [
    "#remove unnecessary variables\n",
    "data_henc = data_henc.drop('town',1)\n",
    "data_henc = data_henc.drop('flat_type',1)\n",
    "data_henc = data_henc.drop('storey_range',1)\n",
    "data_henc = data_henc.drop('flat_model',1)\n",
    "\n",
    "print(data_henc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_month</th>\n",
       "      <th>sales_year</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>resale_price</th>\n",
       "      <th>town_ANG MO KIO</th>\n",
       "      <th>town_BEDOK</th>\n",
       "      <th>town_BISHAN</th>\n",
       "      <th>town_BUKIT BATOK</th>\n",
       "      <th>town_BUKIT MERAH</th>\n",
       "      <th>...</th>\n",
       "      <th>flat_model_Multi Generation</th>\n",
       "      <th>flat_model_New Generation</th>\n",
       "      <th>flat_model_Premium Apartment</th>\n",
       "      <th>flat_model_Premium Apartment Loft</th>\n",
       "      <th>flat_model_Premium Maisonette</th>\n",
       "      <th>flat_model_Simplified</th>\n",
       "      <th>flat_model_Standard</th>\n",
       "      <th>flat_model_Terrace</th>\n",
       "      <th>flat_model_Type S1</th>\n",
       "      <th>flat_model_Type S2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1977</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1977</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1976</td>\n",
       "      <td>47200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_month  sales_year  floor_area_sqm  lease_commence_date  resale_price  \\\n",
       "0            1        1990            31.0                 1977        9000.0   \n",
       "1            1        1990            31.0                 1977        6000.0   \n",
       "2            1        1990            31.0                 1977        8000.0   \n",
       "3            1        1990            31.0                 1977        6000.0   \n",
       "4            1        1990            73.0                 1976       47200.0   \n",
       "\n",
       "   town_ANG MO KIO  town_BEDOK  town_BISHAN  town_BUKIT BATOK  \\\n",
       "0                1           0            0                 0   \n",
       "1                1           0            0                 0   \n",
       "2                1           0            0                 0   \n",
       "3                1           0            0                 0   \n",
       "4                1           0            0                 0   \n",
       "\n",
       "   town_BUKIT MERAH         ...          flat_model_Multi Generation  \\\n",
       "0                 0         ...                                    0   \n",
       "1                 0         ...                                    0   \n",
       "2                 0         ...                                    0   \n",
       "3                 0         ...                                    0   \n",
       "4                 0         ...                                    0   \n",
       "\n",
       "   flat_model_New Generation  flat_model_Premium Apartment  \\\n",
       "0                          0                             0   \n",
       "1                          0                             0   \n",
       "2                          0                             0   \n",
       "3                          0                             0   \n",
       "4                          1                             0   \n",
       "\n",
       "   flat_model_Premium Apartment Loft  flat_model_Premium Maisonette  \\\n",
       "0                                  0                              0   \n",
       "1                                  0                              0   \n",
       "2                                  0                              0   \n",
       "3                                  0                              0   \n",
       "4                                  0                              0   \n",
       "\n",
       "   flat_model_Simplified  flat_model_Standard  flat_model_Terrace  \\\n",
       "0                      0                    0                   0   \n",
       "1                      0                    0                   0   \n",
       "2                      0                    0                   0   \n",
       "3                      0                    0                   0   \n",
       "4                      0                    0                   0   \n",
       "\n",
       "   flat_model_Type S1  flat_model_Type S2  \n",
       "0                   0                   0  \n",
       "1                   0                   0  \n",
       "2                   0                   0  \n",
       "3                   0                   0  \n",
       "4                   0                   0  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_henc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 New Features\n",
    "In this part, we will explore new features that we can add to make our data more valuable. Since the data consists of seven objects, two floats and one integer, the seven objects will be researched and to see which can and will be changed. (Note that adding and dropping variables have been changed to comments, because an error would pop up otherwise. This is because the variable is already added or dropped, thus it cannot be performed again.) <br>\n",
    "\n",
    "***Remaining Lease*** Linear regression will not be able to read the years, since it can see it as another numerical value. Therefore, the remaining lease year is calculated. Once the sales year variable is created, the remaining lease year can be computed by using the following formula: $remaining lease year = 99 - (sales year - lease commence date)$.\n",
    "\n",
    "To check whether the remaining lease variable is correct, the data tail from dataset 5 in the data acquisition is used to compare with the new data. Since only the fifth data set consists of this data, we could use the column for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that data.copy() is used to make a copy of data, which will be used for analysis\n",
    "data_f = data_henc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute remaining lease variable\n",
    "if ('remaining_lease' not in data_f.columns):\n",
    "    data_f['remaining_lease'] = 99 - (data.sales_year - data.lease_commence_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Longtitude and Latitude*** Another interesting feature would be the longtitude and latitude of the street name. Fortunately, Google has such a package to make this possible. Unfortunately, this is only possible for 2,500 data points per day. Since we have 768.629 data points, this task was not possible for us. However, we still want to show that we have tried running the code underneath. Note that this can be seen as a limitation for our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Status code Unknown from https://maps.googleapis.com/maps/api/geocode/json: ERROR - ('Connection aborted.', OSError(\"(32, 'EPIPE')\",))\n"
     ]
    }
   ],
   "source": [
    "#compute longlat variable\n",
    "#data['long_lat'] = geocoder.google(data['street_name']).lating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Area*** Instead of longtitude and latitutde, we have made an extra variable called \"Area\". The Area captures all the town in a specific region, which is based on the information of the official Singapore government site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that data.copy() is used to make a copy of data, which will be used for analysis\n",
    "data_2f = data_f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add area variable\n",
    "data_2f.insert(1,'area',(data['town']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area \n",
      "NORTH         213559\n",
      "WEST          191750\n",
      "CENTRAL       158781\n",
      "NORTH EAST    134634\n",
      "EAST           63067\n",
      "Name: area, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#divide towns into areas\n",
    "data_2f['area'][data_2f.area == 'BUKIT MERAH'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'TOA PAYOH'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'QUEENSTOWN'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'GEYLANG'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'KALLANG/WHAMPOA'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'BISHAN'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'MARINE PARADE'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'CENTRAL AREA'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'BUKIT TIMAH'] = 'CENTRAL'\n",
    "data_2f['area'][data_2f.area == 'TAMPINES'] = 'NORTH'\n",
    "data_2f['area'][data_2f.area == 'YISHUN'] = 'NORTH'\n",
    "data_2f['area'][data_2f.area == 'BEDOK'] = 'NORTH'\n",
    "data_2f['area'][data_2f.area == 'PASIR RIS'] = 'NORTH'\n",
    "data_2f['area'][data_2f.area == 'JURONG WEST'] = 'WEST'\n",
    "data_2f['area'][data_2f.area == 'BUKIT BATOK'] = 'WEST'\n",
    "data_2f['area'][data_2f.area == 'CHOA CHU KANG'] = 'WEST'\n",
    "data_2f['area'][data_2f.area == 'CLEMENTI'] = 'WEST'\n",
    "data_2f['area'][data_2f.area == 'JURONG EAST'] = 'WEST'\n",
    "data_2f['area'][data_2f.area == 'BUKIT PANJANG'] = 'WEST'\n",
    "data_2f['area'][data_2f.area == 'WOODLANDS'] = 'EAST'\n",
    "data_2f['area'][data_2f.area == 'SEMBAWANG'] = 'EAST'\n",
    "data_2f['area'][data_2f.area == 'LIM CHU KANG'] = 'EAST'\n",
    "data_2f['area'][data_2f.area == 'ANG MO KIO'] = 'NORTH EAST'\n",
    "data_2f['area'][data_2f.area == 'HOUGANG'] = 'NORTH EAST'\n",
    "data_2f['area'][data_2f.area == 'SERANGOON'] = 'NORTH EAST'\n",
    "data_2f['area'][data_2f.area == 'SENGKANG'] = 'NORTH EAST'\n",
    "data_2f['area'][data_2f.area == 'PUNGGOL'] = 'NORTH EAST'\n",
    "\n",
    "area_count = data_2f['area'].value_counts()\n",
    "print(\"Area \\n\" +str(area_count))\n",
    "\n",
    "#source: http://www.hdb.gov.sg/cs/infoweb/about-us/history/hdb-towns-your-home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for area\n",
    "dummies = pd.get_dummies(data_2f['area']).rename(columns=lambda x: 'area_' + str(x))\n",
    "data_2f = pd.concat([data_2f, dummies], axis=1)\n",
    "\n",
    "del data_2f['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***GDP*** As an emerging market, Singapore went through rapid economic transform since the 1980s. Together with the growth of international trade, and its increased significance in the financial markets. The increase in GDP, and thereby the income of Singaporeans, will likely have an effect on the resale prices of HDB flats.\n",
    "\n",
    "Source: https://data.worldbank.org/country/singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sales_year  gdp_per_capita\n",
      "60         1990           12638\n",
      "61         1990           12766\n",
      "62         1991           14285\n",
      "63         1991           14504\n",
      "64         1992           16153\n",
      "65         1992           16144\n",
      "66         1993           17916\n",
      "67         1993           18302\n",
      "68         1994           21607\n",
      "69         1994           21578\n",
      "70         1995           25117\n",
      "71         1995           24937\n",
      "72         1996           26073\n",
      "73         1996           26261\n",
      "74         1997           26903\n",
      "75         1997           26387\n",
      "76         1998           22146\n",
      "77         1998           21824\n",
      "78         1999           22044\n",
      "79         1999           21797\n",
      "80         2000           23648\n",
      "81         2000           23794\n",
      "82         2001           21351\n",
      "83         2001           21577\n",
      "84         2002           21400\n",
      "85         2002           22017\n",
      "86         2003           22756\n",
      "87         2003           23574\n",
      "88         2004           25474\n",
      "89         2004           27403\n",
      "90         2005           27891\n",
      "91         2005           29866\n",
      "92         2006           32387\n",
      "93         2006           33580\n",
      "94         2007           37986\n",
      "95         2007           39224\n",
      "96         2008           37932\n",
      "97         2008           39724\n",
      "98         2009           36790\n",
      "99         2009           38578\n",
      "100        2010           46305\n",
      "101        2010           46570\n",
      "102        2011           51743\n",
      "103        2011           53162\n",
      "104        2012           52429\n",
      "105        2012           54431\n",
      "106        2013           54182\n",
      "107        2013           56029\n",
      "108        2014           54890\n",
      "109        2014           56337\n",
      "110        2015           51244\n",
      "111        2015           53630\n",
      "112        2016           51269\n",
      "113        2016           52962\n",
      "114        2017           57722\n",
      "115        2018           57722\n"
     ]
    }
   ],
   "source": [
    "gdp = pd.read_csv('per-capita-gni-and-per-capita-gdp-at-current-market-prices-in-usd-annual.csv', sep =',')\n",
    "idx = (gdp['year'] > 1989)\n",
    "gdp = gdp[idx]\n",
    "gdp = gdp.drop('level_1',axis=1)\n",
    "gdp['sales_year'] = gdp['year']\n",
    "gdp['gdp_per_capita'] = gdp['value']\n",
    "del gdp['year']\n",
    "del gdp['value']\n",
    "print(gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_3f = pd.concat([data_2f,gdp])\n",
    "data_3f = data_2f.merge(gdp[['sales_year','gdp_per_capita']], on='sales_year',how='outer')\n",
    "#data_3f = data_2f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761791, 83)\n",
      "(1502104, 84)\n",
      "Index(['sales_month', 'sales_year', 'floor_area_sqm', 'lease_commence_date',\n",
      "       'resale_price', 'town_ANG MO KIO', 'town_BEDOK', 'town_BISHAN',\n",
      "       'town_BUKIT BATOK', 'town_BUKIT MERAH', 'town_BUKIT PANJANG',\n",
      "       'town_BUKIT TIMAH', 'town_CENTRAL AREA', 'town_CHOA CHU KANG',\n",
      "       'town_CLEMENTI', 'town_GEYLANG', 'town_HOUGANG', 'town_JURONG EAST',\n",
      "       'town_JURONG WEST', 'town_KALLANG/WHAMPOA', 'town_LIM CHU KANG',\n",
      "       'town_MARINE PARADE', 'town_PASIR RIS', 'town_PUNGGOL',\n",
      "       'town_QUEENSTOWN', 'town_SEMBAWANG', 'town_SENGKANG', 'town_SERANGOON',\n",
      "       'town_TAMPINES', 'town_TOA PAYOH', 'town_WOODLANDS', 'town_YISHUN',\n",
      "       'flat_type_1 ROOM', 'flat_type_2 ROOM', 'flat_type_3 ROOM',\n",
      "       'flat_type_4 ROOM', 'flat_type_5 ROOM', 'flat_type_EXECUTIVE',\n",
      "       'flat_type_MULTI GENERATION', 'storey_range_01 TO 03',\n",
      "       'storey_range_04 TO 06', 'storey_range_07 TO 09',\n",
      "       'storey_range_10 TO 12', 'storey_range_13 TO 15',\n",
      "       'storey_range_16 TO 18', 'storey_range_19 TO 21',\n",
      "       'storey_range_22 TO 24', 'storey_range_25 TO 27',\n",
      "       'storey_range_28 TO 30', 'storey_range_31 TO 33',\n",
      "       'storey_range_34 TO 36', 'storey_range_37 TO 39',\n",
      "       'storey_range_40 TO 42', 'storey_range_43 TO 45',\n",
      "       'storey_range_46 TO 48', 'storey_range_49 TO 51', 'flat_model_2-room',\n",
      "       'flat_model_Adjoined flat', 'flat_model_Apartment',\n",
      "       'flat_model_Apertment', 'flat_model_DBSS', 'flat_model_Improved',\n",
      "       'flat_model_Improved-Maisonette', 'flat_model_Maisonette',\n",
      "       'flat_model_Model A', 'flat_model_Model A-Maisonette',\n",
      "       'flat_model_Model A2', 'flat_model_Multi Generation',\n",
      "       'flat_model_New Generation', 'flat_model_Premium Apartment',\n",
      "       'flat_model_Premium Apartment Loft', 'flat_model_Premium Maisonette',\n",
      "       'flat_model_Simplified', 'flat_model_Standard', 'flat_model_Terrace',\n",
      "       'flat_model_Type S1', 'flat_model_Type S2', 'remaining_lease',\n",
      "       'area_CENTRAL', 'area_EAST', 'area_NORTH', 'area_NORTH EAST',\n",
      "       'area_WEST', 'gdp_per_capita'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_2f.shape)\n",
    "print(data_3f.shape)\n",
    "\n",
    "print(data_3f.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Land*** Located on the bottom of the Malaysian peninsula, Singapore is a small city-state with limited land mass. Due to this situation, housing prices tend to be higher than for other countries. To cope with the limited amount of land, the government initiated projects to increase the land mass. From the period 1960 until 2018, the land mass of the country increased with 24% from 581.5 sq/km to 721.5 sq/km.\n",
    "\n",
    "source: https://data.gov.sg/dataset/total-land-area-of-singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = pd.read_csv('total-land-area-of-singapore.csv', sep =',')\n",
    "idx = (land['year'] > 1990)\n",
    "land = land[idx]\n",
    "land['sales_year'] = land['year']\n",
    "del land['year']\n",
    "#print(land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4f = data_3f.merge(land, on='sales_year',how='left')\n",
    "#print(data_4f.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Demand for Rental and Sold Flats*** Besides key metrics regarding the nation’s economy, the Singaporean government also supplies information concerning the demand for rental and sold HDB flats. The data dates back from 1960 until 2016. This data gives insight in the demand from Singaporeans, and is provided in batches of five years. To work with the data, the average of the five years is included per year.\n",
    "\n",
    "source: https://data.gov.sg/dataset/key-stats-since-1960-demand-for-rental-and-sold-flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sales_year', 'demand_for_rental', 'demand_for_owner', 'start_year',\n",
      "       'end_year', 'flat_type', 'demand_for_rental.1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "demand = pd.read_csv('demand-for-rental-and-sold-flats.csv', sep =',')\n",
    "print(demand.columns)\n",
    "demand = demand.drop(columns=['start_year','end_year','flat_type','demand_for_rental.1'],axis=1)\n",
    "#print(demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5f = data_3f.merge(demand, on='sales_year',how='left')\n",
    "#print(data_5f.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761791, 83)\n",
      "(1476880, 84)\n"
     ]
    }
   ],
   "source": [
    "print(data_2f.shape)\n",
    "print(data_3f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Normalizations\n",
    "Data normalization is known as a fundamental preprocessing task to improve the prediction of a model. A normalized dataset might enhances the learning capability with minimum error, since the quality of the data is guaranteed before using any learning algorithm. The data will be scaled in the same range of values for the input features to minize bias [source]. We are using this technique as well to research if this method might improve our scores, since the input are on widely different scales. Two different types of normalization will be used, respectively Z-scoring and Max/Min normalization.\n",
    "\n",
    "[source] S.C. Nayak, B.B. Misra, and H.S. Behera (2016) Impact of Data Normalization on Stock Index Forecasting, p. 257-269 Volume 6 https://pdfs.semanticscholar.org/f412/4953553981e32c39273bb2745a140311d160.pdf\n",
    "\n",
    "#### 3.5.1 Z-scoring\n",
    "Z-scoring normalizes the values of the features according to the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_z = data_2f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scoring\n",
    "data_z[['sales_month', 'sales_year','floor_area_sqm','remaining_lease','lease_commence_date']] = (data_z[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']] - data_z[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']].mean())/data_z[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Max/Min-Normalization\n",
    "This method normalizes the values of the features according to the minimum and maximum of these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = data_2f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max/min\n",
    "data_n[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']] = (data_n[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']] - data_n[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']].min())/(data_n[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']].max() - data_n[['sales_month', 'sales_year', 'floor_area_sqm','remaining_lease','lease_commence_date']].min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Additional One Hot Encoding (Discrete Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_henc_2 = data_2f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for sales_year\n",
    "dummies = pd.get_dummies(data_henc_2['sales_year']).rename(columns=lambda x: 'sy_' + str(x))\n",
    "data_henc_2 = pd.concat([data_henc_2, dummies], axis=1)\n",
    "\n",
    "#one hot encoding for sales_month\n",
    "dummies = pd.get_dummies(data_henc_2['sales_month']).rename(columns=lambda x: 'sm_' + str(x))\n",
    "data_henc_2 = pd.concat([data_henc_2, dummies], axis=1)\n",
    "\n",
    "#one hot encoding for lease_commence_date\n",
    "dummies = pd.get_dummies(data_henc_2['lease_commence_date']).rename(columns=lambda x: 'lcd_' + str(x))\n",
    "data_henc_2 = pd.concat([data_henc_2, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_henc_2 = data_henc_2.drop(columns=['sales_year','sales_month','lease_commence_date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761791, 171)\n"
     ]
    }
   ],
   "source": [
    "print(data_henc_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data Analysis\n",
    "<hr>\n",
    "This part of the report will show algorithms that have been applied to predict the housing prices. We have focused on regressions with different features. This section will first define all the models that will be used and afterwards the results of applying the models to the different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Models\n",
    "In our data analysis, we used different techniques to predict the future resale price. First, the linear regression is discussed. Afterwards, two ensemble learning techniques are introduced. To cover the scope of ensembling, we will  both use a bagging and boosting technique. The random forest bagging technique is introduced first, before moving to the gradient boosting technique. To complete the analysis, and compare the predictive functions, we also introduce the neural network prediction. This is done to validate the research objective of this study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "Linear regression is one of the most common used modeling technique where the dependent variable is continuous and the independent variables can be either continuous or discrete [source]. Since our dataset has the same setting, this technique is used as one of the possible models.\n",
    "\n",
    "[source] B. Patel, 17 Apr. 2017, Predicting house value using regression analysis, https://towardsdatascience.com/regression-analysis-model-used-in-machine-learning-318f7656108a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def lin_reg(data,cv):\n",
    "    start = time.time()\n",
    "\n",
    "    data_input = data.drop('resale_price' ,axis=1)\n",
    "    data_output = data['resale_price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.33, random_state=42)\n",
    "\n",
    "    model_lin_reg = LinearRegression()\n",
    "    model_lin_reg.fit(x_train, y_train)\n",
    "    y_pred_l = model_lin_reg.predict(x_test)\n",
    "    #y_pred_l_train = model_lin_reg.predict(x_train)\n",
    "    \n",
    "    mae_l = mean_absolute_error(y_test, y_pred_l)\n",
    "    #mae_l_train = mean_absolute_error(y_train, y_pred_l_train)\n",
    "    print(\"\\nMAE for Linear Regression is: %.0f\"%mae_l)\n",
    "    #print(\"For the train set: %.0f\" %mae_l_train)\n",
    "    \n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        model_lin_reg_cv = LinearRegression()\n",
    "        scores_lr = cross_val_score(model_lin_reg_cv, data_input, data_output, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        scores_lr = - scores_lr\n",
    "        print(scores_lr)\n",
    "        print(\"CV MAE: %0.2f (+/- %0.2f)\" % (scores_lr.mean(), scores_lr.std() * 2))\n",
    "    \n",
    "    \n",
    "    print('LR Time = %.0f'%(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Random Forest is a learning technique that is part of the bootstrap aggregating techniques, also known as bagging. The technique can be utilised for both classification and regression problems. However, [SOURCE] notes that in case of overfitting data, the random forest will make it even worse. However, when there are enough trees in the forest, the classifier will not easily overfit the model. [SOURCE2] describe random forests as multiple decision trees that are being merged, resulting in a more accurate and stable prediction. \n",
    "\n",
    "SOURCE: Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.\n",
    "SOURCE2: https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Random Forest\n",
    "def random_f(data,version,cv):\n",
    "    start = time.time()\n",
    "    \n",
    "    data_input = data.drop('resale_price' ,axis=1)\n",
    "    data_output = data['resale_price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.33, random_state=42)\n",
    "\n",
    "    model_Forest = RandomForestRegressor()\n",
    "    model_Forest.fit(x_train, y_train)\n",
    "    y_pred_f = model_Forest.predict(x_test)\n",
    "    #y_pred_f_train = model_Forest.predict(x_train)\n",
    "    \n",
    "    mae_f = mean_absolute_error(y_test, y_pred_f)\n",
    "    #mae_f_train = mean_absolute_error(y_train, y_pred_f_train)\n",
    "    print(\"\\nMAE for Random Forest is: %.0f\"%mae_f)\n",
    "    #print(\"For the train set: %.0f\" %mae_f_train)\n",
    "    \n",
    "    cdf_f = r2_score(y_test, y_pred_f)\n",
    "    print('R-squared for Random Forest: %.2f' % cdf_f)\n",
    "\n",
    "    if (version == 1):\n",
    "        importances = model_Forest.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        columns = np.array(list(data_input))\n",
    "        return importances\n",
    "        \n",
    "        # Print the feature ranking\n",
    "        print(\"\\nFeature ranking:\")\n",
    "        \n",
    "        for f in range(x_train.shape[1]):\n",
    "            print(\"%d. %s (%f)\" % (f + 1, columns[indices[f]], importances[indices[f]]))\n",
    "        \n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        scores_rf = cross_val_score(model_Forest, data_input, data_output, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        scores_rf = - scores_rf\n",
    "        print(scores_rf)\n",
    "        print(\"CV MAE: %0.2f (+/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))\n",
    "    \n",
    "    print('RF Time = %.0f'%(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor\n",
    "Gradient boosting is a machine learning technique for both regression and classification problems [SOURCE]. The algorithm is able to produce a decision tree based on input data. Boosting is an ensemble technique, where the predictors are not made independently, but sequentially [SOURCE2]. By including this algorithm, we perform two types of ensembling in our analysis: bagging (random forest) and boosting (gradient boosting).\n",
    "\n",
    "In terms of gradient boosting, we apply three different techniques. The first technique is the standard gradient boosting regressor and the latter two are based on the technique with tweaks in the learning process.\n",
    "\n",
    "\n",
    "SOURCES:\n",
    "Ridgeway, G. (1999). The state of boosting. Computing Science and Statistics, 172-181.\n",
    "\n",
    "Meir, R., & Rätsch, G. (2003). An introduction to boosting and leveraging. In Advanced lectures on machine learning (pp. 118-183). Springer, Berlin, Heidelberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GradientBoostingRegressor\n",
    "def gbr(data,cv):\n",
    "    start = time.time()\n",
    "    \n",
    "    data_input = data.drop('resale_price' ,axis=1)\n",
    "    data_output = data['resale_price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.33, random_state=42)\n",
    "\n",
    "    model_gbr = GradientBoostingRegressor()\n",
    "    model_gbr.fit(x_train, y_train)\n",
    "    y_pred_gbr = model_gbr.predict(x_test)\n",
    "    \n",
    "    mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
    "    print(\"\\nMean Absolute Error for GradientBoostingRegressor is: %.0f\" %mae_gbr) \n",
    "\n",
    "    cdf_gbr = r2_score(y_test, y_pred_gbr)\n",
    "    print('R-squared for GradientBoostingRegressor: %.2f' % cdf_gbr)\n",
    "\n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        scores_gbr = cross_val_score(model_gbr, data_input, data_output, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        scores_gbr = - scores_gbr\n",
    "        print(scores_gbr)\n",
    "        print(\"CV MAE: %0.2f (+/- %0.2f)\" % (scores_gbr.mean(), scores_gbr.std() * 2))\n",
    "     \n",
    "    print('GBR Time = %.0f'%(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AdaBoost\n",
    "def ada(data,cv):\n",
    "    start = time.time()\n",
    "    \n",
    "    data_input = data.drop('resale_price' ,axis=1)\n",
    "    data_output = data['resale_price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.33, random_state=42)\n",
    "\n",
    "    model_abr = AdaBoostRegressor()\n",
    "    model_abr.fit(x_train, y_train)\n",
    "    y_pred_abr = model_abr.predict(x_test)\n",
    "    \n",
    "    mae_abr = mean_absolute_error(y_test, y_pred_abr)\n",
    "    print(\"\\nMean Absolute Error for AdaBoost is: %.0f\" %mae_abr)\n",
    "    \n",
    "    cdf_abr = r2_score(y_test, y_pred_abr)\n",
    "    print('R-squared for AdaBoost: %.2f' % cdf_abr)\n",
    " \n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        scores_ada = cross_val_score(model_abr, data_input, data_output, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        scores_ada = - scores_ada\n",
    "        print(scores_ada)\n",
    "        print(\"CV MAE: %0.2f (+/- %0.2f)\" % (scores_ada.mean(), scores_ada.std() * 2))\n",
    "    \n",
    "    print('Ada Time = %.0f'%(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run XG Boost\n",
    "def xg_boost(data,cv):\n",
    "    start = time.time()\n",
    "    \n",
    "    data_input = data.drop('resale_price' ,axis=1)\n",
    "    data_output = data['resale_price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.33, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "    dtest = xgb.DMatrix(x_test, label = y_train)\n",
    "    param = {\n",
    "        'max_depth': 3,  # the maximum depth of each tree. Try with max_depth: 2 to 10.\n",
    "        'eta': 0.3,  # the training step for each iteration. Try with ETA: 0.1, 0.2, 0.3...\n",
    "        'silent': 1,  # logging mode - quiet\n",
    "        'objective': 'reg:linear'}  # defines the loss function to be minimized  \n",
    "    num_round = 20  # the number of training iterations. Try with num_round around few hundred!\n",
    "    #----------------\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    y_pred_xgb = bst.predict(dtest)\n",
    "    best_preds = np.asarray([np.argmax(line) for line in y_pred_xgb])\n",
    "\n",
    "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    print(\"\\nMean Absolute Error for XGBoost is: %.0f\" %mae_xgb)\n",
    "    #xgb.plot_importance(bst)\n",
    "    #plt.show()\n",
    "\n",
    "    cdf_xgb = r2_score(y_test, y_pred_xgb)\n",
    "    print('R-squared for XGBoost: %.2f' % cdf_xgb)\n",
    "    \n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        scores_xgb = cross_val_score(xgb.train, data_input, data_output, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        scores_xgb = - scores_xgb\n",
    "        print(scores_xgb)\n",
    "        print(\"CV MAE: %0.2f (+/- %0.2f)\" % (scores_xgb.mean(), scores_xgb.std() * 2))\n",
    "    \n",
    "    print('XGB Time = %.0f'%(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "Predictive Neural Network is a powerful predictive modeling technique, which can learn to perform predictive tasks. It can for example be trained to predict numerical values, such as housing prices. As mentioned in the introduction, previous studies showed that this technique might perform better compared to a regression model. However, since Singapore is a quasi-open market, Neural Network is used to research if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def neural(data,cv):\n",
    "    start = time.time()\n",
    "    \n",
    "    data_input = data.drop('resale_price' ,axis=1)\n",
    "    data_output = data['resale_price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.33, random_state=42)\n",
    "\n",
    "    model_n = MLPRegressor()\n",
    "    model_n.fit(x_train, y_train)\n",
    "    y_pred_n = model_n.predict(x_test)\n",
    "    \n",
    "    mae_n = mean_absolute_error(y_test, y_pred_n)\n",
    "    print(\"\\nMean Absolute Error for Neural Network is: %.0f\" %mae_n)  \n",
    "    \n",
    "    cdf_n = r2_score(y_test, y_pred_n)\n",
    "    print('R-squared for Neural Network: %.2f' % cdf_n)\n",
    "    \n",
    "    if (cv == 1):\n",
    "        cv = ShuffleSplit(n_splits=3, test_size=0.33, random_state=42)\n",
    "        scores = cross_val_score(model_n, data_input, data_output, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        scores = - scores\n",
    "        print(scores)\n",
    "        print(\"CV MAE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    print('Neural Time = %.0f'%(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analysis\n",
    "The analysis section will show the Mean Absolute Error and R-squared of every model with different scenarios. \n",
    "\n",
    "***Mean Absolute Error***\n",
    "\n",
    "Formula: <br>\n",
    "$$MAE = \\frac{1}{n}\\sum_{j=1}^{n}|y_j - \\hat{y}_j|$$\n",
    "\n",
    "***Coefficient of Determination***\n",
    "The coefficient of determination, also known as $R^2$, is the proportion variance between the dependent variable and the prediction from the independent variable. This coefficient ranges from 0 to 1, where 1 means that there is no variance between the predicted and actual.\n",
    "\n",
    "Formula:<br> \n",
    "$$R^2 = 1 - \\frac{\\sum(y - \\hat{y})^2}{\\sum(y - \\bar{y})^2}$$\n",
    "\n",
    "where $y$ is the actual value, $\\hat{y}$ is the predicted value of y, and $\\bar{y}$ is the mean value of y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 1: Only data encoding\n",
    "Some columns are still of 'object' type and need to be changed to int or float in order to run XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_month              int64\n",
      "sales_year               int64\n",
      "town                     int64\n",
      "flat_type                int64\n",
      "storey_range             int64\n",
      "floor_area_sqm         float64\n",
      "flat_model               int64\n",
      "lease_commence_date      int64\n",
      "resale_price           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(data_enc.dtypes)\n",
    "data_enc['flat_type'] = pd.to_numeric(data_enc['flat_type'])\n",
    "data_enc['storey_range'] = pd.to_numeric(data_enc['storey_range'])\n",
    "data_enc['flat_model'] = pd.to_numeric(data_enc['flat_model'])\n",
    "data_enc['town'] = pd.to_numeric(data_enc['town'])\n",
    "print(data_enc.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly different algorithms and features, we use a sample size of the full train data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = data_enc.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 1222104978\n",
      "[  1.09201779e+16   4.60056157e+16   2.89980905e+10]\n",
      "CV MAE: 18975274203502268.00 (+/- 39252751419199512.00)\n",
      "LR Time = 2\n",
      "\n",
      "MAE for Random Forest is: 15973\n",
      "R-squared for Random Forest: 0.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-a1df38eaaa08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxg_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneural\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-148-2029b46be137>\u001b[0m in \u001b[0;36mrandom_f\u001b[0;34m(data, version, cv)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mscores_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_Forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mscores_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscores_rf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lin_reg(data_enc,1)\n",
    "random_f(data_enc,0,1)\n",
    "gbr(data_enc,1)\n",
    "xg_boost(data_enc,0)\n",
    "neural(data_enc,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenorio 2: Analysis on one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_henc.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 47967\n",
      "R-squared for Linear Regression: 0.80\n",
      "[ 55523.06253656  49459.87404337  43044.16671053  67857.85820959\n",
      "  96245.58648405]\n",
      "\n",
      "CV MAE: 62426.11 (+/- 37573.35)\n",
      "\n",
      "LR Time = 32\n",
      "\n",
      "MAE for Random Forest is: 15874\n",
      "R-squared for Random Forest: 0.97\n",
      "\n",
      "CV MAE: 56892.06 (+/- 65887.97)\n",
      "\n",
      "RF Time = 341\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_henc,1)\n",
    "random_f(data_henc,0,1)\n",
    "#gbr(data_sample,1)\n",
    "#xg_boost(data_sample,1)\n",
    "#neural(data_sample,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 3: Analysis on one new feature (remaining lease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_f.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 47969\n",
      "[ 47967.84234952  48116.02335898  47996.2386813 ]\n",
      "CV MAE: 48026.70 (+/- 128.43)\n",
      "LR Time = 19\n",
      "\n",
      "MAE for Random Forest is: 15900\n",
      "R-squared for Random Forest: 0.97\n",
      "[ 15870.00850873  15892.01256976  15853.29174657]\n",
      "CV MAE: 15871.77 (+/- 31.71)\n",
      "RF Time = 336\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_f,1)\n",
    "random_f(data_f,0,1)\n",
    "#gbr(data_f,1)\n",
    "#xg_boost(data_f,1)\n",
    "#neural(data_f,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores improved and thus we will keep this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 4: Analysis on two new features (remaining lease and area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_2f.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 47967\n",
      "\n",
      "CV MAE for LR is: 98571776\n",
      "[  5.55347464e+04   4.95112696e+04   4.29991840e+04   6.78804067e+04\n",
      "   4.92643553e+08]\n",
      "\n",
      "CV MAE: 98571895.67 (+/- 394071657.39)\n",
      "\n",
      "LR Time = 50\n",
      "\n",
      "MAE for Random Forrest is: 15738\n",
      "\n",
      "CV MAE: 56892.00 (+/- 67088.69)\n",
      "\n",
      "RF Time = 345\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_2f,1)\n",
    "random_f(data_2f,0,1)\n",
    "#gbr(data_2f,1)\n",
    "#xg_boost(data_2f,1)\n",
    "#neural(data_2f,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 5: Analysis on three new features (remaining lease, area and GDP per capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_3f.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 47967\n",
      "\n",
      "CV MAE for LR is: 98571776\n",
      "[  5.55347464e+04   4.95112696e+04   4.29991840e+04   6.78804067e+04\n",
      "   4.92643553e+08]\n",
      "\n",
      "CV MAE: 98571895.67 (+/- 394071657.39)\n",
      "\n",
      "LR Time = 50\n",
      "\n",
      "MAE for Random Forrest is: 15738\n",
      "\n",
      "CV MAE: 56892.00 (+/- 67088.69)\n",
      "\n",
      "RF Time = 345\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_3f,1)\n",
    "random_f(data_3f,0,1)\n",
    "#gbr(data_3f,1)\n",
    "#xg_boost(data_3f,1)\n",
    "#neural(data_3f,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 6: Analysis on three new features (remaining lease, area, GDP per capita and land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_4f.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 47967\n",
      "\n",
      "CV MAE for LR is: 98571776\n",
      "[  5.55347464e+04   4.95112696e+04   4.29991840e+04   6.78804067e+04\n",
      "   4.92643553e+08]\n",
      "\n",
      "CV MAE: 98571895.67 (+/- 394071657.39)\n",
      "\n",
      "LR Time = 50\n",
      "\n",
      "MAE for Random Forrest is: 15738\n",
      "\n",
      "CV MAE: 56892.00 (+/- 67088.69)\n",
      "\n",
      "RF Time = 345\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_4f,1)\n",
    "random_f(data_4f,0,1)\n",
    "#gbr(data_4f,1)\n",
    "#xg_boost(data_4f,1)\n",
    "#neural(data_4f,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 7: Analysis on four new features (remaining lease, area, GDP per capita, land and demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_5f.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 47967\n",
      "\n",
      "CV MAE for LR is: 98571776\n",
      "[  5.55347464e+04   4.95112696e+04   4.29991840e+04   6.78804067e+04\n",
      "   4.92643553e+08]\n",
      "\n",
      "CV MAE: 98571895.67 (+/- 394071657.39)\n",
      "\n",
      "LR Time = 50\n",
      "\n",
      "MAE for Random Forrest is: 15738\n",
      "\n",
      "CV MAE: 56892.00 (+/- 67088.69)\n",
      "\n",
      "RF Time = 345\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_5f,1)\n",
    "random_f(data_5f,0,1)\n",
    "#gbr(data_5f,1)\n",
    "#xg_boost(data_5f,1)\n",
    "#neural(data_5f,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 8: Analysis on Normalization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_z.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n### z-scoring ###')\n",
    "lin_reg(data_z,1)\n",
    "random_f(data_z,0,1)\n",
    "#gbr(data_z,1)\n",
    "#xg_boost(data_z,1)\n",
    "#neural(data_z,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 9: Analysis on max-min normalization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_n.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n### max/min ###')\n",
    "lin_reg(data_n,1)\n",
    "random_f(data_n,0,1)\n",
    "#gbr(data_n,1)\n",
    "#xg_boost(data_n,1)\n",
    "#neural(data_n,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nothing happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 10: Addition One hot Encoding of Months and Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_henc_2.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 29559\n",
      "LR Time = 9\n",
      "\n",
      "MAE for Random Forrest is: 16084\n",
      "RF Time = 145\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_henc_2,1)\n",
    "random_f(data_henc_2,0,1)\n",
    "#gbr(data_henc_2,1)\n",
    "#xg_boost(data_henc_2,1)\n",
    "#neural(data_henc_2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its horsecrap, will not do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = data_2f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761791, 83)\n"
     ]
    }
   ],
   "source": [
    "print(data_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = data_r.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Random Forest is: 15725\n",
      "R-squared for Random Forest: 0.97\n"
     ]
    }
   ],
   "source": [
    "importances = random_f(data_r,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761791, 10)\n"
     ]
    }
   ],
   "source": [
    "#indices = np.argsort(importances)[::-1]\n",
    "columns = np.array(list(data_r.drop('resale_price',1)))\n",
    "\n",
    "for i in range(0,len(columns)):\n",
    "    feature = columns[i]\n",
    "    weight = importances[i]\n",
    "    if (weight < 0.005):\n",
    "        del data_r[feature]\n",
    "        \n",
    "print(data_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE for Linear Regression is: 53504\n",
      "R-squared for Linear Regression: 0.75\n",
      "\n",
      "CV MAE for LR is: 67267\n",
      "[  56148.96950623   53980.03231626   49540.52757144   74387.40058907\n",
      "  102306.47657642]\n",
      "\n",
      "CV MAE: 67272.68 (+/- 38913.72)\n",
      "\n",
      "LR Time = 4\n",
      "\n",
      "MAE for Random Forest is: 24652\n",
      "R-squared for Random Forest: 0.93\n",
      "\n",
      "CV MAE: 60318.27 (+/- 63016.36)\n",
      "\n",
      "RF Time = 100\n"
     ]
    }
   ],
   "source": [
    "lin_reg(data_r,1)\n",
    "random_f(data_r,0,1)\n",
    "#gbr(data_r,1)\n",
    "#xg_boost(data_r,1)\n",
    "#neural(data_r,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deleting any features with low importance only made the result worse\n",
    "- some overall not important at all but maybe very important for the few datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Split Datasets according to Periods\n",
    "\n",
    "Following our assumption in the previous chapter, we also try to run the regressions on seperate datasets to research whether the accuracy will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the average price per square meter in the sales years, periods can be identified. The first period identified is the economic growth from 1990 until 1997 [13]. The \"Asian Crisis\" of 1997-1998 affected Singapore and other emerging markets, which is visible from the decline in resale price in the data [14]. In the subsequent years, Singapore had a stable growth in economic terms, but coped with the economic slowdown in the US, Japan and the EU. Combined with the SARS outbreak in 2003, the resale prices remained relatively stable until 2007. According to [15], the HDB resale prices from 2007 onwards grew even faster than the private property market. [15] argues that the increase in price is the result of an increase in median income of Singaporeans. \n",
    "\n",
    "\n",
    "Splitting the dataset in these periods could help to predict the resale prices of HDB in Singapore. We thereby assume that the resale prices of data in the first period (i.e. 1990-1997) will be less accurate to predict the resale price in 2018. This is based on both economic motives, as well as demographic motives (e.g. increased population and land mass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the datasets based on the periods described\n",
    "data_period1 = data_2f.loc[data['sales_year'].isin(['1990','1991','1992','1993','1994','1995','1996','1997','1998'])]\n",
    "data_period2 = data_2f.loc[data['sales_year'].isin(['1999','2000','2001','2002','2003','2004','2005','2006','2007'])]\n",
    "data_period3 = data_2f.loc[data['sales_year'].isin(['2008','2009','2010','2011','2012','2013''2014','2015','2016','2017','2018'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample_p1 = data_period1.sample(frac=0.1)\n",
    "#data_sample_p2 = data_period2.sample(frac=0.1)\n",
    "#data_sample_p3 = data_period3.sample(frac=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230238, 83)\n",
      "(309490, 83)\n",
      "(189870, 83)\n"
     ]
    }
   ],
   "source": [
    "periods = [data_period1,data_period2,data_period3]\n",
    "for i in range(0,3):\n",
    "    period = periods[i]\n",
    "    print(period.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### For 1990-1998 ###\n",
      "\n",
      "MAE for Random Forest is: 13955\n",
      "R-squared for Random Forest: 0.98\n",
      "[ 13919.90539571  13949.29864603  13988.00869607]\n",
      "CV MAE: 13952.40 (+/- 55.78)\n",
      "RF Time = 65\n",
      "\n",
      "### For 1999-2007 ###\n",
      "\n",
      "MAE for Random Forest is: 13109\n",
      "R-squared for Random Forest: 0.96\n",
      "[ 13130.03719051  13086.40567397  13111.15103569]\n",
      "CV MAE: 13109.20 (+/- 35.73)\n",
      "RF Time = 84\n",
      "\n",
      "### For 2008-2018 ###\n",
      "\n",
      "MAE for Random Forest is: 21205\n",
      "R-squared for Random Forest: 0.94\n",
      "[ 21093.12909408  21335.68599991  21127.06180391]\n",
      "CV MAE: 21185.29 (+/- 214.49)\n",
      "RF Time = 53\n"
     ]
    }
   ],
   "source": [
    "period_names = ['1990-1998','1999-2007','2008-2018']\n",
    "\n",
    "for i in range(0,3):\n",
    "    print('\\n### For',period_names[i],'###')\n",
    "    period = periods[i]\n",
    "    #lin_reg(period,1)\n",
    "    random_f(period,0,1)\n",
    "    #gbr(period,1)\n",
    "    #xg_boost(period,1)\n",
    "    #neural(period,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Absolute Error: 15462.30\n",
      "Overall R-squared: 0.96\n"
     ]
    }
   ],
   "source": [
    "overall_mae = (13944*230238 + 13119*309490 + 21123*189870)/(230238+309490+189870)\n",
    "print('Overall Mean Absolute Error: %.2f' % overall_mae)\n",
    "\n",
    "overall_rsq = (0.98*230238 + 0.96*309490 + 0.94*189870)/(230238+309490+189870)\n",
    "print('Overall R-squared: %.2f' % overall_rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusions, Limitations & Future Research\n",
    "<hr>\n",
    "Add here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "<hr>\n",
    "Add here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
